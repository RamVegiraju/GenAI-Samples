{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc9c5e2-2d27-4d6d-8bed-6dbb67a39954",
   "metadata": {},
   "source": [
    "# SageMaker Inference Components Deployment\n",
    "\n",
    "In this notebook we'll utilize SageMaker Inference Components to deploy a Llama and BART model on a singular endpoint. This endpoint we will then utilize in our chatbot for both QnA from Llama 7B Chat and BART for the summarization portion.\n",
    "\n",
    "### Models Being Utilized\n",
    "- [Llama-7B-Chat](https://huggingface.co/TheBloke/Llama-2-7B-Chat-fp16): For the QnA aspect of chatbot.\n",
    "- [Fine-Tuned BART Model](https://huggingface.co/knkarthick/MEETING_SUMMARY): A fine-tuned BART model on the HuggingFace Hub. This has been fine-tuned on SAMSUM dataset as well.\n",
    "    - License: [Apache 2.0](https://choosealicense.com/licenses/apache-2.0/). No changes done to the model just using the base model provided on the HF Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e776fbe-5d69-4ac1-943e-91214d2abfd3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8c21e-529c-45b3-9c7b-ddfaa8f2ca0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05201b87-3a89-4fd0-87bf-41b6dd85f389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "#Setup\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "boto_session = boto3.session.Session()\n",
    "s3 = boto_session.resource('s3')\n",
    "region = boto_session.region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Role ARN: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "# client setup\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dabf44-271d-42ed-ac77-c32296d542a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# endpoint config name\n",
    "epc_name = \"ic-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(f\"Endpoint Config Name: {epc_name}\")\n",
    "\n",
    "# Container Parameters, increase health check for LLMs: \n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.12xlarge\" # 4 GPUs available per instance\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "# Setting up managed AutoScaling at endpoint level\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 2\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n",
    "\n",
    "# Endpoint Config Creation\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=epc_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            # can set to least outstanding or random: https://aws.amazon.com/blogs/machine-learning/minimize-real-time-inference-latency-by-using-amazon-sagemaker-routing-strategies/\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15362c-f117-4052-9ef7-5e72cf016045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Endpoint Creation\n",
    "endpoint_name = \"ic-ep-chatbot\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=epc_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b960a32-db02-4742-a3dc-aab3d1a3a5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Monitor creation\n",
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(15)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117d972-4671-4c96-bb6d-c46747e50656",
   "metadata": {},
   "source": [
    "### Llama 7B Chat IC Creation\n",
    "\n",
    "For Llama 7B Chat we just follow the ready made guide here with the LMI Container: https://github.com/deepjavalibrary/djl-demo/blob/2a5152f578f5954b8b68acdee18eed4e2a75c81f/aws/sagemaker/large-model-inference/sample-llm/rollingbatch_llama_7b_chat.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71860367-44cd-40f0-869c-dacde56227a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=MPI\n",
    "option.model_id=TheBloke/Llama-2-7B-Chat-fp16\n",
    "option.task=text-generation\n",
    "option.trust_remote_code=true\n",
    "option.tensor_parallel_degree=1\n",
    "option.max_rolling_batch_size=32\n",
    "option.rolling_batch=lmi-dist\n",
    "option.dtype=fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e99d73-2f50-4e1f-99b0-66650dfd7ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "from djl_python.huggingface import HuggingFaceService\n",
    "from djl_python import Output\n",
    "from djl_python.encode_decode import encode, decode\n",
    "from transformers import AutoTokenizer\n",
    "import logging\n",
    "import json\n",
    "import types\n",
    "\n",
    "_service = HuggingFaceService()\n",
    "\n",
    "def custom_parse_input(self, inputs):\n",
    "    input_data = []\n",
    "    input_size = []\n",
    "    parameters = []\n",
    "    errors = {}\n",
    "    # used for chat completion\n",
    "    if self.tokenizer is None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.hf_configs.model_id_or_path)\n",
    "    batch = inputs.get_batches()\n",
    "    for i, item in enumerate(batch):\n",
    "        try:\n",
    "            content_type = item.get_property(\"Content-Type\")\n",
    "            input_map = decode(item, content_type)\n",
    "        except Exception as e:  # pylint: disable=broad-except\n",
    "            logging.warning(f\"Parse input failed: {i}\")\n",
    "            input_size.append(0)\n",
    "            errors[i] = str(e)\n",
    "            continue\n",
    "        # Chat message masssaging\n",
    "        chat = input_map.pop(\"chat\", [])\n",
    "        if len(chat) != 0:\n",
    "            formatted_str = self.tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "            input_data.extend([formatted_str])\n",
    "        else:\n",
    "            input_data.extend([\"\"])\n",
    "        input_size.append(1)\n",
    "        # End of massaging\n",
    "        _param = input_map.pop(\"parameters\", {})\n",
    "        if not \"seed\" in _param:\n",
    "            # set server provided seed if seed is not part of request\n",
    "            if item.contains_key(\"seed\"):\n",
    "                _param[\"seed\"] = item.get_as_string(key=\"seed\")\n",
    "        for _ in range(input_size[i]):\n",
    "            parameters.append(_param)\n",
    "\n",
    "    return input_data, input_size, parameters, errors, batch\n",
    "\n",
    "\n",
    "def chat_output_formatter(token, first_token, last_token, details, generated_tokens):\n",
    "    \"\"\"\n",
    "    json output formatter\n",
    "\n",
    "    :return: formatted output\n",
    "    \"\"\"\n",
    "    json_encoded_str = f\"{{\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"\" if first_token else \"\"\n",
    "    json_encoded_str = f\"{json_encoded_str}{json.dumps(token.text, ensure_ascii=False)[1:-1]}\"\n",
    "    if last_token:\n",
    "        if details:\n",
    "            details_str = f\"\\\"details\\\": {json.dumps(details, ensure_ascii=False)}\"\n",
    "            json_encoded_str = f\"{json_encoded_str}\\\", {details_str}}}\"\n",
    "        else:\n",
    "            json_encoded_str = f\"{json_encoded_str}\\\"}}\"\n",
    "\n",
    "    return json_encoded_str\n",
    "\n",
    "\n",
    "def handle(inputs):\n",
    "    if not _service.initialized:\n",
    "        props = inputs.get_properties()\n",
    "        props[\"output_formatter\"] = chat_output_formatter\n",
    "        _service.initialize(inputs.get_properties())\n",
    "        # replace parse_input\n",
    "        _service.parse_input = types.MethodType(custom_parse_input, _service)\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # initialization request\n",
    "        return None\n",
    "\n",
    "    return _service.inference(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc546f0-7b32-4ca7-9a07-c39c89ae29b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "rm mymodel.tar.gz\n",
    "mv serving.properties mymodel/\n",
    "mv model.py mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7f525-89cd-482f-93ac-8f956713b664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sagemaker_session.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )\n",
    "print(f\"Image being used: {image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5e495-b775-4517-a896-1180eef54063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "llama_model_name = name_from_base(f\"Llama-7b-chat\")\n",
    "print(llama_model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=llama_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": image_uri, \"ModelDataUrl\": code_artifact},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df1280-afe8-4234-89a8-6fafee1ebdb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama7b_ic_name = \"llama7b-chat-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "# llama inference component reaction\n",
    "create_llama_ic_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName=llama7b_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": llama_model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            # need just one GPU for llama 7b chat\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    # can setup autoscaling for copies, each copy will retain the hardware you have allocated\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")\n",
    "\n",
    "print(\"IC Llama Arn: \" + create_llama_ic_response[\"InferenceComponentArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e5b2f4-7b76-4ae4-9324-58489eea8246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "describe_ic_llama_response = client.describe_inference_component(\n",
    "    InferenceComponentName=llama7b_ic_name)\n",
    "\n",
    "while describe_ic_llama_response[\"InferenceComponentStatus\"] == \"Creating\":\n",
    "    describe_ic_llama_response = client.describe_inference_component(InferenceComponentName=llama7b_ic_name)\n",
    "    print(describe_ic_llama_response[\"InferenceComponentStatus\"])\n",
    "    time.sleep(100)\n",
    "print(describe_ic_llama_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24884a48-9e2b-42d0-8326-bde347f3be31",
   "metadata": {},
   "source": [
    "#### Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d0411-ebd7-4ca9-a094-e617d2c9af3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "content_type = \"application/json\"\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I am software engineer looking to learn more about machine learning.\"},\n",
    "]\n",
    "\n",
    "payload = {\"chat\": chat, \"parameters\": {\"max_tokens\":256, \"do_sample\": True}}\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=llama7b_ic_name, #specify IC name\n",
    "    ContentType=content_type,\n",
    "    Body=json.dumps(payload),\n",
    "    )\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(type(result['content']))\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38165b2a-3a17-4ed3-9421-f3dbcfeb6fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"initial chat: {chat}\")\n",
    "chat.append(result) #add dialogue to chat\n",
    "print(f\"updated chat: {chat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4758c-6687-4d4a-a199-ab192e4c41c5",
   "metadata": {},
   "source": [
    "### BART Summarization Model IC Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35791e5-c1b4-45b3-b902-0f15768847c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "bart_model_name = name_from_base(f\"bart-summarization\")\n",
    "print(bart_model_name)\n",
    "\n",
    "# replace with your region if needed\n",
    "hf_transformers_image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04'\n",
    "\n",
    "# env variables\n",
    "env = {'HF_MODEL_ID': 'knkarthick/MEETING_SUMMARY',\n",
    "      'HF_TASK':'summarization',\n",
    "      'SAGEMAKER_CONTAINER_LOG_LEVEL':'20',\n",
    "      'SAGEMAKER_REGION':'us-east-1'}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=bart_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    # in this case no model data point directly towards HF Hub\n",
    "    PrimaryContainer={\"Image\": hf_transformers_image_uri, \n",
    "                      \"Environment\": env},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece2572-0e9a-45bd-ab0b-1ccec5dd44fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bart_ic_name = \"bart-summarization-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "# BART inference component reaction\n",
    "create_bart_ic_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName=bart_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": bart_model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            # will reserve one GPU\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"NumberOfCpuCoresRequired\": 8,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    # can setup autoscaling for copies, each copy will retain the hardware you have allocated\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")\n",
    "\n",
    "print(\"IC BART Arn: \" + create_bart_ic_response[\"InferenceComponentArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4269fa-2de8-4dbb-b58e-59fd03776791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "describe_ic_bart_response = client.describe_inference_component(\n",
    "    InferenceComponentName=bart_ic_name)\n",
    "\n",
    "while describe_ic_bart_response[\"InferenceComponentStatus\"] == \"Creating\":\n",
    "    describe_ic_bart_response = client.describe_inference_component(InferenceComponentName=bart_ic_name)\n",
    "    print(describe_ic_bart_response[\"InferenceComponentStatus\"])\n",
    "    time.sleep(100)\n",
    "print(describe_ic_bart_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1f689-9e24-4c33-a235-10e3ab679252",
   "metadata": {},
   "source": [
    "#### Sample Inference\n",
    "Note we want to feed the conversation we have with Llama into this IC for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c088a-d9a2-49ea-88e6-e6ca8e60c467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt template (can use langchain to make cleaner if you want)\n",
    "text = ''''''\n",
    "\n",
    "# prepare payload\n",
    "for resp in chat:\n",
    "    if resp['role'] == \"user\":\n",
    "        text += f\"Ram: {resp['content']}\\n\"\n",
    "    elif resp['role'] == \"assistant\":\n",
    "        text += f\"AI: {resp['content']}\\n\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9dccf1-febd-4c7b-b698-1df3681836cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"inputs\": text}\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=bart_ic_name, #specify IC name\n",
    "    ContentType=content_type,\n",
    "    Body=json.dumps(payload),\n",
    "    )\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result[0]['summary_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
