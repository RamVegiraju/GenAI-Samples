{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b72388-dac8-43f2-9966-34378bd52d46",
   "metadata": {},
   "source": [
    "# Bedrock Implementation of Medium Analyzer\n",
    "\n",
    "In this notebook we will use Claude for the LLM via Bedrock and the Titan Embeddings Model to build out a simple RAG Workflow orchestrated by LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693b743-f751-4fb0-9c76-71ddd6f08c16",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Can use any Python environment that has Boto3 access to the Bedrock models.\n",
    "\n",
    "### Credits\n",
    "Bedrock OSS RAG Reference: https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571901e-6d15-4118-b311-4d15f91b1837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install langchain>=0.1.11\n",
    "#%pip install pypdf==4.1.0\n",
    "#%pip install langchain-community faiss-cpu==1.8.0 tiktoken==0.6.0 sqlalchemy==2.0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd577f86-3dbf-4826-bf7a-9d4df28ec3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import langchain\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "boto3_bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef354f7f-9a15-434f-8afd-293076530ce1",
   "metadata": {},
   "source": [
    "## Sample Boto3 Inference With Claude V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2da9a9-2660-44bd-82a4-addc3447bbf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_id = 'anthropic.claude-v2'\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "prompt_data = \"\"\"Human: Write me a small paragraph saying nice things about me.\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "print(prompt_data)\n",
    "\n",
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n",
    "response = boto3_bedrock.invoke_model(\n",
    "    body=body, modelId=model_id, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "print(response_body.get(\"completion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7682a7d-4cf4-4408-ba03-d1f47ea110e5",
   "metadata": {},
   "source": [
    "## Embeddings & Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ace06-4fbf-486e-9a73-19991d201634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# where our embeddings will be stored\n",
    "store = LocalFileStore(\"./cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db004b-5103-4e39-9d7b-2d6273aadb24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate a loader: this loads our data, use PDF in this case\n",
    "loader = PyPDFDirectoryLoader(\"sagemaker-articles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec645e0-de63-4f86-aee2-2f7b4c3a5c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# by default the PDF loader both loads and splits the documents for us\n",
    "pages = loader.load_and_split()\n",
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd7bf95-7116-4298-bff5-9644e6e9b63d",
   "metadata": {},
   "source": [
    "## Chain Creation\n",
    "\n",
    "We instantiate the LLM and Embeddings model we are using and point towards our vector database with the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c526b4-81f3-40ca-876c-3c4ef8a26c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# - create the LLM and Embeddings Models\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={'max_tokens_to_sample':200})\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "\n",
    "# pass in our vector store\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    bedrock_embeddings,\n",
    "    store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c713e-8ba3-43f1-8d3e-b4bf55c23633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create vector store, we use FAISS in this case\n",
    "vector_store = FAISS.from_documents(pages, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27d224-bdab-439f-8dca-7e388b855f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is the entire retrieval system\n",
    "medium_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e289082-1ebd-47b9-aec4-a2bb973d2837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper method to structure prompt template, optionally use langchain prompt template\n",
    "def fill_prompt(template, human_text):\n",
    "    # Replace the placeholder 'Human:' with the provided human_text\n",
    "    filled_prompt = template.replace(\"Human:\", f\"Human: {human_text}\")\n",
    "    return filled_prompt\n",
    "\n",
    "# Your template\n",
    "prompt_data = \"\"\"Human:\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "# sample input\n",
    "human_input = \"You are an incredible friend, always supportive and kind.\"\n",
    "result = fill_prompt(prompt_data, human_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b0765-e522-4105-bca1-49b0a499b9d5",
   "metadata": {},
   "source": [
    "## Sample Inference with RAG and Vanilla Bedrock Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451d0e7-8cf7-46ba-86a2-9e69e52fd54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_prompts = [\"What does Ram Vegiraju write about?\",\n",
    "                 \"What is Amazon SageMaker?\",\n",
    "                 \"What is Amazon SageMaker Inference?\",\n",
    "                 \"What are the different hosting options for Amazon SageMaker?\",\n",
    "                 \"What is Serverless Inference with Amazon SageMaker?\",\n",
    "                 \"What's the difference between Multi-Model Endpoints and Multi-Container Endpoints?\",\n",
    "                 \"What SDKs can I use to work with Amazon SageMaker?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e5621-9215-4bfa-8c7d-1cf050e87e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for prompt in sample_prompts:\n",
    "    print(prompt)\n",
    "    print()\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Vanilla Bedrock Response\")\n",
    "    print(\"------------------------------------\")\n",
    "    print()\n",
    "    prompt_template = fill_prompt(prompt_data, prompt)\n",
    "    body = json.dumps({\"prompt\": prompt_template, \"max_tokens_to_sample\": 500})\n",
    "    response = boto3_bedrock.invoke_model(\n",
    "        body=body, modelId=model_id, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    print(response_body.get(\"completion\"))\n",
    "    print()\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"RAG Enabled Response\")\n",
    "    print(\"------------------------------------\")\n",
    "    response_rag = medium_qa_chain({\"query\":prompt})\n",
    "    print(response_rag['result'])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
