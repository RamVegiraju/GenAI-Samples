{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452a5262-e505-4b44-ac4d-02b8f38ee0bc",
   "metadata": {},
   "source": [
    "## Medium SageMaker Analyzer\n",
    "\n",
    "In this example we see how good my Medium articles on SageMaker are. We build a RAG based application by feeding some of my popular Medium articles (stored in sagemaker-articles) to a QA Chain. We've pre saved these articles in the sagemaker-articles directory as pdfs.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- OpenAI API Key: This is the LLM we use in this case\n",
    "\n",
    "Documentation:\n",
    "\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa\n",
    "\n",
    "Additional Resources/Guides\n",
    "\n",
    "- https://deci.ai/blog/retrieval-augmented-generation-using-langchain/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae96d4e-d7e1-4443-9895-9dd366556e49",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e44928-d9e8-4c2d-a3fd-e23cf47d5d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbee184-1219-445b-a00e-e6526f722579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea631d-4169-47e2-9416-9168342de047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'Enter your API Key here'\n",
    "print(os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1f2a2-42f5-4eb8-a4ab-37237393f431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01046c6-a5bd-4ea1-b60a-a1e34c675868",
   "metadata": {},
   "source": [
    "### RAG Setup\n",
    "\n",
    "We need the following to implement RAG with LangChain:\n",
    "\n",
    "- <b>Storage</b>: A local store for our data we are providing for our RAG application. To scale up you can utilize other stores such as S3 as this gets larger\n",
    "- <b>Embeddings model</b>: To create embedding out of the provided data, we use OpenAI Embeddings\n",
    "- <b>Vector Store</b>: Store model embeddings, FAISS in this case\n",
    "- <b>Chain</b>: Stitches together these different components, our LLM models is OpenAI in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598323b-9cea-482b-ae2d-e5de58d1f717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# where our embeddings will be stored\n",
    "store = LocalFileStore(\"./cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224d4bb-6aee-4c12-ae9b-bfb95430eac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate a loader: this loads our data, use PDF in this case\n",
    "loader = PyPDFDirectoryLoader(\"sagemaker-articles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaedd45-17c0-41d5-a1e8-8359abc53d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# by default the PDF loader both loads and splits the documents for us\n",
    "pages = loader.load_and_split()\n",
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481ab5c-7ce3-4c69-a301-eaa93a94f618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate embedding model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# pass in our vector store\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings_model,\n",
    "    store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfbc559-ba02-4da0-805e-ec5f61fb4e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create vector store, we use FAISS in this case\n",
    "vector_store = FAISS.from_documents(pages, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05270f90-7199-462d-8e52-469a5979a9d3",
   "metadata": {},
   "source": [
    "### Chain Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92e7cb-b9ea-4055-abc2-b6b438819932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is the entire retrieval system\n",
    "medium_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(),\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea010a2-c75f-41b4-b1fe-386efba91e1a",
   "metadata": {},
   "source": [
    "### Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e1599-bd8a-4a74-82b4-4fd7dc2ed34a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_prompts = [\"What does Ram Vegiraju write about?\",\n",
    "                 \"What is Amazon SageMaker?\",\n",
    "                 \"What is Amazon SageMaker Inference?\",\n",
    "                 \"What are the different hosting options for Amazon SageMaker?\",\n",
    "                 \"What is Serverless Inference with Amazon SageMaker?\",\n",
    "                 \"What's the difference between Multi-Model Endpoints and Multi-Container Endpoints?\",\n",
    "                 \"What SDKs can I use to work with Amazon SageMaker?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676517ac-7d43-4fe2-9174-bac6003783c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for prompt in sample_prompts:\n",
    "    response = medium_qa_chain({\"query\":prompt})\n",
    "    print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
