{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1755ba-4bfa-4f64-9975-b939dde6eefb",
   "metadata": {},
   "source": [
    "## LLMOps Pipeline with SageMaker Pipelines, JumpStart, and FMEval\n",
    "\n",
    "In this example we will take a look at building an LLMOps Pipeline utilizing SageMaker Pipelines, JumpStart, and FMEval. We will use JumpStart to fine-tune a Llama7B model, the FMEval package for evaluating the fine-tuned model, and Pipelines for the MLOps portion of the example.\n",
    "\n",
    "### Credits/Reference\n",
    "- <b>[SageMaker JumpStart Llama 2 Fine-Tuning Guide](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/llama-2-finetuning.ipynb)</b>: We'll use this example as a base for the first step of fine-tuning our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d56af9-92a7-474a-965e-d08166a713b6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll download the public [dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) and utilize it for a summarization use-case. We filter the dataset for the summarization samples and push the data to S3 for both training and inference/evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc03aca-23bc-4502-847c-b5a9e6ae8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bc6fc-a1be-4505-8b0c-fcb250b7b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48eaf58-054b-4dc4-9831-61097ab9b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import sagemaker\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"2.*\"\n",
    "\n",
    "# dolly dataset\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")\n",
    "\n",
    "# test dataset\n",
    "train_and_test_dataset[\"test\"].to_json(\"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf23c3a-eb55-4f24-afea-c3046d084d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939abef-5957-44d8-befe-ad2c9d31bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "test_data_file = \"test.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset\"\n",
    "test_data_location = f\"s3://{output_bucket}/test_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "S3Uploader.upload(test_data_file, test_data_location)\n",
    "print(f\"Training data: {train_data_location}\")\n",
    "print(f\"Test data: {test_data_location}\")\n",
    "print(f\"Output bucket: {output_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72be57-7a72-458c-a20c-0ce4b5242a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = test_data_location + \"/\"\n",
    "print(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e038ff4c-c818-4619-b34f-a9e3d192f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {test_data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9617148f-e1f3-482f-9fcd-f582e545848d",
   "metadata": {},
   "source": [
    "## Pipelines Setup\n",
    "\n",
    "For this example we have two main steps: training and model/inference evaluation.\n",
    "\n",
    "1. <b> Training </b>: We pull the S3 dataset and fine-tune utilizing SageMaker JumpStart with Llama2.\n",
    "2. <b> Inference/Evaluation </b>. We use SageMaker Clarify/FMEval library to perform evaluation on the summarization use-case. Before we can run evaluation we perform inference on the test dataset to run FMEval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb5c79-6f3c-4b70-8e9a-68cda67b9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "instance_type = ParameterString(name=\"TrainInstanceType\", default_value=\"ml.c5.18xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e6f34-9bd1-4453-aede-c9aa0e5d74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set path to config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261934e-dda7-4f80-9b00-0eedd70648fa",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271eb28d-5e34-4851-a4d3-b79d6a869174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one\n",
    "@step(\n",
    "    name = \"train-deploy\",\n",
    "    instance_type = instance_type,\n",
    "    keep_alive_period_in_seconds=300\n",
    ")\n",
    "def train_deploy(train_data_path: str, model_id: str = \"meta-textgeneration-llama-2-7b\", model_version: str = \"2.*\") -> str:\n",
    "    import sagemaker\n",
    "    from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "    estimator = JumpStartEstimator(\n",
    "        model_id=model_id,\n",
    "        model_version=model_version,\n",
    "        environment={\"accept_eula\": \"true\"},\n",
    "        disable_output_compression=True, \n",
    "    )\n",
    "\n",
    "    # reducing epoch count to 1 for example sake\n",
    "    print(\"--------------\")\n",
    "    print(\"Starting training\")\n",
    "    print(\"--------------\")\n",
    "    estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"1\", max_input_length=\"1024\")\n",
    "    estimator.fit({\"training\": train_data_path})\n",
    "\n",
    "    # deploy fine-tuned model\n",
    "    print(\"--------------\")\n",
    "    print(\"Starting deployment\")\n",
    "    print(\"--------------\")\n",
    "    finetuned_predictor = estimator.deploy()\n",
    "    endpoint_name = finetuned_predictor.endpoint_name\n",
    "    return endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346ef27-5d55-4a53-a626-daa7701ad651",
   "metadata": {},
   "source": [
    "### Inference & Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928ceda-96ac-4427-8ae3-6e55bce660f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to prepare datapoint for inference\n",
    "def prepare_payload(datapoint: dict) -> dict:\n",
    "    template = {\n",
    "        \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "        \"completion\": \" {response}\",\n",
    "    }\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df862d13-cd12-4fe1-bf4a-0aea75de0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step two\n",
    "@step(\n",
    "    name = \"evaluate-infer\",\n",
    "    instance_type = instance_type,\n",
    "    keep_alive_period_in_seconds=300\n",
    ")\n",
    "def evaluate(endpoint_name: str, output_bucket: str = output_bucket, test_data_file: str = \"test.jsonl\",\n",
    "            key_path: str = \"test_dataset/test.jsonl\") -> str:\n",
    "    import os\n",
    "    import boto3\n",
    "    import jsonlines\n",
    "    import json\n",
    "    import fmeval\n",
    "    from fmeval.data_loaders.data_config import DataConfig\n",
    "    from fmeval.constants import MIME_TYPE_JSONLINES\n",
    "    from fmeval.eval_algorithms.summarization_accuracy import SummarizationAccuracy\n",
    "    os.environ[\"PARALLELIZATION_FACTOR\"] = \"1\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "    # download test dataset for inference\n",
    "    s3.download_file(output_bucket, key_path, test_data_file)\n",
    "    print(\"--------------\")\n",
    "    print(\"Downloaded test dataset file\")\n",
    "    print(\"--------------\")\n",
    "    input_file = \"test.jsonl\"\n",
    "    output_file = \"results.jsonl\"\n",
    "    content_type = \"application/json\"\n",
    "    \n",
    "    print(\"--------------\")\n",
    "    print(\"Starting Inference\")\n",
    "    print(\"--------------\")\n",
    "    with jsonlines.open(input_file) as input_fh, jsonlines.open(output_file, \"w\") as output_fh:\n",
    "        for i, datapoint in enumerate(input_fh, start=1):\n",
    "            instruction = datapoint[\"instruction\"]\n",
    "            context = datapoint[\"context\"]\n",
    "            summary = datapoint[\"response\"]\n",
    "            payload = prepare_payload(datapoint)\n",
    "            response = runtime.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(payload), \n",
    "                                   ContentType=content_type, CustomAttributes='accept_eula=true')\n",
    "            result = json.loads(response['Body'].read().decode())[0]['generation']\n",
    "            line = {\"instruction\": instruction, \"context\": context, \"summary\": summary, \"model_output\": result}\n",
    "            output_fh.write(line)\n",
    "\n",
    "            # evaluate just 20 datapoints for example\n",
    "            if i == 20:\n",
    "                break\n",
    "\n",
    "    print(\"--------------\")\n",
    "    print(\"Starting Evaluation\")\n",
    "    print(\"--------------\")\n",
    "    config = DataConfig(\n",
    "        dataset_name=\"dolly_summary_model_outputs\",\n",
    "        dataset_uri=\"results.jsonl\",\n",
    "        dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "        model_input_location=\"instruction\",\n",
    "        target_output_location=\"summary\",\n",
    "        model_output_location=\"model_output\"\n",
    "    )\n",
    "    eval_algo = SummarizationAccuracy()\n",
    "    eval_output = eval_algo.evaluate(dataset_config=config, save=True)\n",
    "    res = json.dumps(eval_output, default=vars, indent=4)\n",
    "    serialized_data = json.loads(res)\n",
    "    # print metrics to CW logs, realistically push to somewhere to visualize\n",
    "    for item in serialized_data:\n",
    "        for key, value in item.items():\n",
    "            print(f\"Key: {key}, Value: {value}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c96550-7e0d-4122-a96d-8132072e5ad7",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a266e1-9551-4cba-89e0-dcf466b54a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch together pipeline\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "endpoint_name = train_deploy(train_data_location)\n",
    "eval_metrics = evaluate(endpoint_name)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"llm-train-eval-pipeline\",\n",
    "    parameters=[\n",
    "        instance_type\n",
    "    ],\n",
    "    steps=[\n",
    "        eval_metrics,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964df8aa-d578-492d-a7b7-50a18be57a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "execution.describe()\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f19b77-50a7-4402-9546-5e1a76bdfcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
